{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb7e0c3-6ca3-4f56-a794-85b0e5250040",
   "metadata": {},
   "source": [
    "# Parameter Estimation for the Vasicek Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318497bc-aef1-4e95-9244-bb8b5762c935",
   "metadata": {},
   "source": [
    "This Jupyter notebook will explain the procedure of calibrating the Vasicek model and it's parameters to real world data. This model has been introduced in the original article __Vasicek, Oldrich \"An equilibrium characterization of the term structure.\" Journal of financial economics 5.2 (1977): 177-188__ which will be our main reference for the underlying model during this assignment.\n",
    "The vasicek model is a model for \n",
    "simulating interest rates and it is often used because of the mean-reverting properties. In general, for parameters $a,b,\\sigma \\in \\mathbb{R}$ and $T<\\infty$ the model dynamics are described by an Ornstein-Uhlenbeck process:\n",
    "$$ dr_t = a(b-r_t)dt + \\sigma dW_t \\quad , t \\in [0,T]$$\n",
    "\n",
    "where $W_t$ is a standard Wiener process/Brownian Motion. The parameter effects can be summarized by noting\n",
    "\n",
    "<ul style=\"font-family: 'Times New Roman', serif; margin-left: 1.2em;\">\n",
    "  <li> $b \\in \\mathbb{R}$  , the long term mean (the drift component is attracted to this parameter)</li>\n",
    "  <li> $a > 0$, the speed/rate at which the mean reversion happens</li>\n",
    "  <li> $\\sigma > 0$, the instantaneous volatility (or the \"amount of noise\").</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "We will not explain the procedure of solving this equation and the underlying theory since this is out of scope of this assignment. The necessary theory for Ito calculus and SDE solutions can be found in __Karatzas, Ioannis, and Steven Shreve  Brownian motion and stochastic calculus springer, 2014__. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c759326-87cd-4836-8f49-cc4b091860df",
   "metadata": {},
   "source": [
    "## Ornstein-Uhlenbeck process properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d07b7-912a-4a74-b6e8-2bc73dcec4ab",
   "metadata": {},
   "source": [
    "The Ornstein-Uhlenbeck (**OU**) process has a solution of the form\n",
    "\n",
    "$$r_t = r_0 e^{-at} + b(1-e^{-at}) + \\sigma e^{-at}\\int_{0}^{t}e^{as}dW_s \\; .$$\n",
    "\n",
    "A very useful fact (that we will not prove but can be proven by comparing characteristic/moment generating function as they uniquely define the entire distribution) is that \n",
    "\n",
    "$$ \\int_{0}^{t}f(s)dW_s \\sim \\mathcal{N}\\left(0,\\int_{0}^{t}f(s)^2ds\\right) \\; .$$\n",
    "\n",
    "We therefore obtain by integral computations that $r_t$ has a normal distribution with\n",
    "\n",
    "$$ r_t \\sim \\mathcal{N}\\left(r_0 e^{-at} + b(1-e^{-at}) \\;, \\; \\frac{\\sigma^2}{2a}(1-e^{-2at}) \\right)\\quad .$$\n",
    "\n",
    "By using the so called **Flow** property of strong SDE solutions, we can even start this process at time $s$ instead of $0$ and obtain as a conditional distribution:\n",
    "\n",
    "$$ r_t | r_s \\sim \\mathcal{N}\\left(r_s e^{-a(t-s)} + b(1-e^{-a(t-s)}) \\;, \\; \\frac{\\sigma^2}{2a}(1-e^{-2a(t-s)}) \\right) \\quad .$$\n",
    "\n",
    "If we now use an equally distanced discretization $\\{t_1,\\dots,t_n \\; | \\; \\text{  s.t.  } \\; t_{i+1}-t_i = \\frac{T}{n}  =: \\Delta t \\} $ for $[0,T]$ and observe realizations of $(r_t)_{ 0\\leq t \\leq T}$ at times $t_i$ we obtain by the previous observation:\n",
    "\n",
    "$$ r_{t_{i+1}} | r_{t_{i}} \\sim \\mathcal{N}\\left(r_s e^{-a\\Delta t} + b(1-e^{-a\\Delta t}) \\;, \\; \\frac{\\sigma^2}{2a}(1-e^{-2a\\Delta t}) \\right) \\quad .$$\n",
    "\n",
    "This shows that this conditional distribution only cares about the distance in time, not the exact point in time as it only depends on $\\Delta t$. This allows us to use a series of real world observations of $r_t$ and fit a Normal Distribution with **constant** mean and variance. This makes it much simpler in the future, as other discretizations would not produce constant mean and variance. Note that this is only obtained because of the nice structure the solution of this very specific SDE has, solutions to more complex SDE's might not offer a similar result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee5c0e-6606-4a1c-8c54-070f21fc2dca",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d369d5c-61af-4690-9e4a-3a6538f16879",
   "metadata": {},
   "source": [
    " We will focus on the case of having a conditional density function $f(x|y \\; ;\\theta) $ with $\\theta \\in \\Theta$ being the parameters we want to estimate. We assume to have $n$ iid samples $x_1, \\dots, x_n$ that we assume to be distributed according to our density function, i.e. being produced by our underlying model. The MLE is the parameter $\\hat{\\theta}_{\\text{MLE}}$ that maximizes the likelihood function\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{n}f(x_i|x_{i-1}\\; ; \\theta) \\; .$$\n",
    "\n",
    "This term however is numerically unstable as it is prone to either be extremely small or large due to chain multiplication. It is common practice to therefore apply a strictly monotonous function that makes the likelihood function easier to handle, mostly the $log$ function since it transforms the product into a sum:\n",
    "\n",
    "$$ \\log L(\\theta) = \\sum_{i=1}^{n}\\log f(x_i|x_{i-1}\\; ; \\theta) \\; .$$\n",
    "\n",
    "Often times instead of maximizing this term one instead minimizes the negative log-likelihood function which is obviously equivalent.\n",
    "In our case we know the conditional density $f$ since we just saw that our increments are distributed according to a normal distribution.\n",
    "We want to estimate $a,b, \\sigma$ which means we can set $\\theta = (a,b,\\sigma)$.\n",
    "This means we can explicitly calculate its logarithm\n",
    "\n",
    "$$ \\log f(x_i|x_{i-1}\\; ; a,b,\\sigma) = -\\frac{1}{2}\\log(2\\pi \\nu) - \\frac{(x_i - \\mu_{i-1})^2}{2\\nu}$$\n",
    "with \n",
    "$$ \\nu = \\frac{\\sigma^2}{2a}(1-e^{-2a\\Delta t}) \\quad \\text{and} \\quad \\mu_{i-1} = x_{i-1} e^{-a\\Delta t} + b(1-e^{-a\\Delta t}) \\; .$$\n",
    "\n",
    "We can define the log-likelihood function now as a function in Python as well with corresponding inputs. The goal is to minimize it's output using Pythons <u><code>scipy</code></u> package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a215b386-3e90-4830-b57e-d3a40845d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast package to handle array wide computations:\n",
    "import numpy as np\n",
    "# Import enhanced type checking functionalities for arrays to use as input datatype for observations:\n",
    "from numpy.typing import NDArray\n",
    "# Package that we will use for minimization:\n",
    "import scipy\n",
    "# Package for data structures and reading/saving data:\n",
    "import pandas as pd\n",
    "\n",
    "def NegLLVasicek(params: tuple, r:NDArray[np.float64], dt: float) -> float:\n",
    "    \"\"\" Function that evaluates the negative log-likelihood function specifically in our underlying Vasicek model.\"\"\"\n",
    "    # Assign just as we described above\n",
    "    a, b, sigma = params\n",
    "\n",
    "    # This term is observed multiple times, compute once for efficiency\n",
    "    phi = np.exp(-a * dt)\n",
    "\n",
    "    # Compute variance (independant for r!)\n",
    "    var = (sigma**2 / (2 * a)) * (1 - phi**2)\n",
    "    \n",
    "    # compute conditional means (for the entire array! mu is an array with the conditional means!)\n",
    "    mu = phi * r[:-1] + (1 - phi) * b\n",
    "    \n",
    "    # gaussian log-likelihood (just as term above but taking the sum directly)\n",
    "    ll = -0.5 * np.sum(np.log(2 * np.pi * var) + ((r[1:] - mu) ** 2) / var)\n",
    "    return -ll  # negative log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655526e-cdb3-42d2-bd78-b13747fcf788",
   "metadata": {},
   "source": [
    "In this notebook we will not talk about the data cleanup and preparation again, this is being described in detail in <u><code>./data/data_cleaning.py </code></u>. \n",
    "Here we will directly import the prepared csv data and continue with the MLE precedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef70e2cb-5fae-4e79-8b09-6289fc4817a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rate_pct\n",
      "date                \n",
      "2001-01-02    0.0492\n",
      "2001-01-03    0.0514\n",
      "2001-01-04    0.0503\n",
      "2001-01-05    0.0493\n",
      "2001-01-08    0.0494\n",
      "2001-01-09    0.0498\n",
      "2001-01-10    0.0510\n",
      "2001-01-11    0.0514\n",
      "2001-01-12    0.0525\n",
      "2001-01-15    0.0525\n"
     ]
    }
   ],
   "source": [
    "# Use pandas functionality to read in the cleaned dgs10 data:\n",
    "data = pd.read_csv(\"../data/dgs10_clean.csv\", index_col=\"date\")\n",
    "\n",
    "# Print first 10 rows to take a look:\n",
    "print(data.head(10))\n",
    "\n",
    "# define day-delta on the basis of which we will simulate the SDE dynamics (discretization step assumed to be average business day per year):\n",
    "dt = 1 / 252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5286ccf6-a8e4-449e-b95a-808b4c4eded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Extract the values of the 'data' Dataframe into an array (DataFrame method returns np.array)\n",
    "r = data.values\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb0cc6-fe56-4162-9a07-406d6ddc2041",
   "metadata": {},
   "source": [
    "Our goal is to use <u><code>scipy.optimize.minimize </code></u>. Let us get some information on how to use it. Looking at the documentation the most important inputs that we wil have \n",
    "to specify are \n",
    "\n",
    "<ul style=\"font-family: 'Times New Roman', serif; margin-left: 1.2em;\">\n",
    "  <li><code>fun</code> , the python function to be minimized</li>\n",
    "  <li><code>x_0</code>, the starting value(s) in which we shall minimize, in our case starting values for $a,b,\\sigma$</li>\n",
    "  <li><code>args</code> , additional arguments for us $r$ and $dt$</li>\n",
    "  <li><code>method</code> , an optimization method that will be used (a lot of choices)</li>\n",
    "  <li><code>bounds</code> , bounds for the parameters</li>\n",
    "</ul>\n",
    "\n",
    "The specification using <code>args</code> allows us to set which inputs are taken every iteration of the optimization algorithm, for us this is $r$ and $dt$ as we optimize over using these\n",
    "as our observations and additional set parameter, and which function inputs are the ones <code>scipy</code> is supposed to find a minimum over. For the method we will use the so called\n",
    "__L-BFGS-B__ method that is memory efficient and regularly used in parameter estimation problems just as ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb9fffbf-4c56-48ba-aff8-070c0f069d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some starting parameters\n",
    "start = np.array([1, 1, 1])\n",
    "\n",
    "# Justification for these bounds are given by the parameter description in the introduction to this model\n",
    "bnds = [(1e-9, 10.0), # bounds for a --> a > 0\n",
    "        (None, None), # bounds for b --> unconstrained\n",
    "        (1e-9, 10.0) # bounds for sigma --> sigma > 0\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52514bba-8a73-4e4c-af08-26b36893baef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimates: a = 0.3039, b = 0.0312341, sigma = 0.00911741, fun-value = -37835.1\n"
     ]
    }
   ],
   "source": [
    "# This will return an 'OptimizeResult' object that has multiple attributes that we call later\n",
    "opt = scipy.optimize.minimize(\n",
    "    fun=NegLLVasicek, \n",
    "    x0=start, \n",
    "    args=(r, dt), \n",
    "    method=\"L-BFGS-B\", \n",
    "    bounds=bnds\n",
    ")\n",
    "\n",
    "# opt.success is an attribute that signals if the procedure has run without errors\n",
    "if not opt.success:\n",
    "    raise RuntimeError(\"MLE optimization failed: \" + opt.message)\n",
    "\n",
    "# Multi-assignment from opt.x attribute containing optimization result array + opt.fun gives log-likelihood function value for optimized values\n",
    "a_mle, b_mle, sigma_mle = opt.x\n",
    "print(\n",
    "    \"MLE estimates: a = %.6g, b = %.6g, sigma = %.6g, fun-value = %.6g\"\n",
    "    % (a_mle, b_mle, sigma_mle, opt.fun)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3accba52-6c5f-4de0-97fa-56026b9e792b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
